{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prognostika\n",
    "- Comprehensive ML to predict hard drive failures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis and Case Study\n",
    "- This notebook is written to keep track of stastical data and condcut case study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import dask.dataframe as dd\n",
    "from collections import Counter\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.colors\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', None)  # Shows all rows\n",
    "pd.set_option('display.max_columns', None)  # Shows all columns\n",
    "pd.set_option('display.width', None)  # Uses the maximum width to display each column\n",
    "pd.set_option('display.max_colwidth', None)  # Shows full length of the data in columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generate data based on given parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for years and quarters\n",
    "years = ['2016']\n",
    "quarters = ['Q1']  # Define which quarters to process\n",
    "required_features = ['serial_number', 'model', 'failure', 'date']\n",
    "all_data = []\n",
    "\n",
    "# Get the script directory\n",
    "script_dir = os.path.dirname(os.path.abspath(''))\n",
    "\n",
    "# Join quarters to form the part of the file name\n",
    "quarter_part = '_'.join(quarters) if quarters else 'full_year'\n",
    "\n",
    "# Construct the pickle file name\n",
    "pickle_file = os.path.join(script_dir, 'output', f\"{'_'.join(years)}_{quarter_part}.pkl\")\n",
    "\n",
    "# Quarter date ranges\n",
    "quarter_ranges = {\n",
    "    'Q1': ('01-01', '03-31'),\n",
    "    'Q2': ('04-01', '06-30'),\n",
    "    'Q3': ('07-01', '09-30'),\n",
    "    'Q4': ('10-01', '12-31')\n",
    "}\n",
    "\n",
    "# Check if the pickle file already exists\n",
    "if os.path.exists(pickle_file):\n",
    "    print(f\"Loading data from {pickle_file}...\")\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "    print(\"Data loaded successfully from pickle file.\")\n",
    "else:\n",
    "    # Loop over each year\n",
    "    for y in tqdm(years, desc=\"Analyzing years\"):\n",
    "        # Construct the path to get all files for the year\n",
    "        path = os.path.join(script_dir, 'HDD_dataset', y, '*.csv')\n",
    "        files = glob.glob(path)\n",
    "        \n",
    "        # Filter files based on quarters\n",
    "        filtered_files = []\n",
    "        for q in quarters:\n",
    "            start_date = f\"{y}-{quarter_ranges[q][0]}\"\n",
    "            end_date = f\"{y}-{quarter_ranges[q][1]}\"\n",
    "            filtered_files += [f for f in files if start_date <= os.path.basename(f)[:10] <= end_date]\n",
    "        \n",
    "        # Loop over each file that falls within the selected quarters\n",
    "        for f in tqdm(filtered_files, desc=f\"Analyzing files in {y} {', '.join(quarters)}\"):\n",
    "            try:\n",
    "                # Read the first row to determine the columns\n",
    "                temp_df = pd.read_csv(f, nrows=1)\n",
    "                all_columns = temp_df.columns\n",
    "\n",
    "                # Identify columns containing the keyword 'raw'\n",
    "                raw_columns = [col for col in all_columns if 'raw' in col]\n",
    "\n",
    "                # Combine required features with raw columns\n",
    "                features = required_features + raw_columns\n",
    "\n",
    "                # Read the CSV file with specified features\n",
    "                data = pd.read_csv(f, usecols=features, parse_dates=['date'], low_memory=False)\n",
    "                data['failure'] = data['failure'].astype('int')\n",
    "                all_data.append(data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {f}: {e}\")\n",
    "\n",
    "    # Combine all data into a single DataFrame if there is any data\n",
    "    if all_data:\n",
    "        df = pd.concat(all_data, ignore_index=True)\n",
    "        df.set_index(['serial_number', 'date'], inplace=True)\n",
    "        df.sort_index(inplace=True)\n",
    "        \n",
    "        # Save the DataFrame to a pickle file\n",
    "        os.makedirs(os.path.join(script_dir, 'output'), exist_ok=True)\n",
    "        with open(pickle_file, 'wb') as f:\n",
    "            pickle.dump(df, f)\n",
    "        print(f\"Data concatenated and saved to {pickle_file} successfully.\")\n",
    "    else:\n",
    "        print(\"No data to concatenate.\")\n",
    "\n",
    "# Backup the DataFrame\n",
    "backup_df = df.copy()\n",
    "\n",
    "# Display the DataFrame\n",
    "print('Data concatenated successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = backup_df.copy() # Restore the original DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split into failure vs non-failure drives serial number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the DataFrame into failures and non-failures\n",
    "failure_df = df[df['failure'] == 1]\n",
    "non_failure_df = df[df['failure'] == 0]\n",
    "\n",
    "print(f\"Data split into failures and non-failures.\")\n",
    "print(f\"Failures: {failure_df.shape[0]} records.\")\n",
    "print(f\"Non-failures: {non_failure_df.shape[0]} records.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manufacturer-level and Model-level analysis\n",
    "- The values of SMART stats and whether or not they are reported varies from vendor to vendor. Furthermore, simply including vendor as a feature may or may not work for all kinds of prediction models. Therefore, it may be a good idea to analyze the data in a vendor specific way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can first not run his.\n",
    "# Total number of records in the dataset\n",
    "total_records = df.shape[0]\n",
    "\n",
    "# Splitting the DataFrame by manufacturer and calculating percentages\n",
    "seagate_df = df[df[\"model\"].str.startswith(\"S\")]\n",
    "seagate_percentage = (seagate_df.shape[0] / total_records) * 100\n",
    "print(f\"Seagate records: {seagate_df.shape}, {seagate_percentage:.2f}% of total\")\n",
    "\n",
    "hgst_df = df[df[\"model\"].str.startswith(\"HG\")]\n",
    "hgst_percentage = (hgst_df.shape[0] / total_records) * 100\n",
    "print(f\"HGST records: {hgst_df.shape}, {hgst_percentage:.2f}% of total\")\n",
    "\n",
    "toshiba_df = df[df[\"model\"].str.startswith(\"T\")]\n",
    "toshiba_percentage = (toshiba_df.shape[0] / total_records) * 100\n",
    "print(f\"Toshiba records: {toshiba_df.shape}, {toshiba_percentage:.2f}% of total\")\n",
    "\n",
    "wdc_df = df[df[\"model\"].str.startswith(\"W\")]\n",
    "wdc_percentage = (wdc_df.shape[0] / total_records) * 100\n",
    "print(f\"WDC records: {wdc_df.shape}, {wdc_percentage:.2f}% of total\")\n",
    "\n",
    "hitachi_df = df[df[\"model\"].str.startswith(\"Hi\")]\n",
    "hitachi_percentage = (hitachi_df.shape[0] / total_records) * 100\n",
    "print(f\"Hitachi records: {hitachi_df.shape}, {hitachi_percentage:.2f}% of total\")\n",
    "\n",
    "\n",
    "# Define the data\n",
    "labels = ['Seagate', 'HGST', 'Toshiba', 'WDC', 'Hitachi']\n",
    "sizes = [seagate_percentage, hgst_percentage, toshiba_percentage, wdc_percentage, hitachi_percentage]\n",
    "\n",
    "# Create a pie chart\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle\n",
    "ax1.axis('equal')  \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Show some statistical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to assign vendor based on model prefix\n",
    "def assign_vendor(model):\n",
    "    if model.startswith(\"S\"):\n",
    "        return \"Seagate\"\n",
    "    elif model.startswith(\"HG\"):\n",
    "        return \"HGST\"\n",
    "    elif model.startswith(\"T\"):\n",
    "        return \"Toshiba\"\n",
    "    elif model.startswith(\"W\"):\n",
    "        return \"WDC\"\n",
    "    elif model.startswith(\"Hi\"):\n",
    "        return \"Hitachi\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "    \n",
    "\n",
    "# Calculate total failures and non-failures for each model\n",
    "model_failures = failure_df['model'].value_counts()\n",
    "model_non_failures = non_failure_df['model'].value_counts()\n",
    "\n",
    "# Calculate total counts and percentages\n",
    "total_counts = df['model'].value_counts()\n",
    "failure_percentages = (model_failures / total_counts * 100).fillna(0)  # fill NaN with 0 where no failures are recorded\n",
    "\n",
    "# Sort models by total failure count in descending order\n",
    "sorted_models = model_failures.sort_values(ascending=False)\n",
    "\n",
    "# Prepare data for display\n",
    "data = []\n",
    "for model in sorted_models.index:\n",
    "    fail_count = sorted_models[model]\n",
    "    non_fail_count = model_non_failures.get(model, 0)\n",
    "    total_count = total_counts[model]\n",
    "    total_percent = (total_count / total_counts.sum()) * 100\n",
    "    fail_percent = (fail_count / total_count) * 100 if total_count else 0\n",
    "\n",
    "    data.append({\n",
    "        \"Model\": model,\n",
    "        \"Failure count\": fail_count,\n",
    "        \"Non-failure count\": non_fail_count,\n",
    "        \"Total count\": total_count,\n",
    "        \"Total percent\": total_percent,  # Keeping as float\n",
    "        \"Fail percent\": fail_percent,   # Keeping as float\n",
    "        \"Vendor\": assign_vendor(model)\n",
    "    })\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "report_df = pd.DataFrame(data)\n",
    "\n",
    "# Set display options (optional, for full display in Jupyter Notebook)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(report_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot top 10 data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract top 10 models by failure percentage\n",
    "top10_fail_percent = report_df.nlargest(10, 'Fail percent')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "barplot = sns.barplot(x='Fail percent', y='Model', data=top10_fail_percent, palette='viridis')\n",
    "plt.title('Top 10 Models by Failure Percentage')\n",
    "plt.xlabel('Failure Percentage')\n",
    "plt.ylabel('Model')\n",
    "\n",
    "# Adding annotations to each bar\n",
    "for p in barplot.patches:\n",
    "    width = p.get_width()    # Get the width of the bar\n",
    "    plt.text(x=width + 0.5,  # Place text at 0.5 units to the right of the bar\n",
    "             y=p.get_y() + p.get_height() / 2,  # Vertically align text in the middle of the bar\n",
    "             s='{:.2f}%'.format(width),  # The text to display\n",
    "             va='center')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Extract top 10 models by total failure count\n",
    "top10_fail_count = report_df.nlargest(10, 'Failure count')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "barplot = sns.barplot(x='Failure count', y='Model', data=top10_fail_count, palette='magma')\n",
    "plt.title('Top 10 Models by Total Failure Count')\n",
    "plt.xlabel('Total Failure Count')\n",
    "plt.ylabel('Model')\n",
    "\n",
    "# Adding annotations to each bar\n",
    "for p in barplot.patches:\n",
    "    width = p.get_width()    # Get the width of the bar\n",
    "    plt.text(x=width + 3,    # Place text at 3 units to the right of the bar\n",
    "             y=p.get_y() + p.get_height() / 2,  # Vertically align text in the middle of the bar\n",
    "             s=int(width),  # The text to display, converted to int for clean display\n",
    "             va='center')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Analysis by manufacture instead of extact model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total number of records in the dataset\n",
    "total_records = df.shape[0]\n",
    "\n",
    "# Splitting the DataFrame by manufacturer and calculating percentages\n",
    "vendors = [\"Seagate\", \"HGST\", \"Toshiba\", \"WDC\", \"Hitachi\"]\n",
    "vendor_data = {}\n",
    "\n",
    "for vendor in vendors:\n",
    "    vendor_df = df[df[\"model\"].str.startswith(vendor[0])]\n",
    "    vendor_percentage = (vendor_df.shape[0] / total_records) * 100\n",
    "    vendor_data[vendor] = {\n",
    "        \"records\": vendor_df.shape[0],\n",
    "        \"percentage\": vendor_percentage\n",
    "    }\n",
    "    print(f\"{vendor} records: {vendor_df.shape}, {vendor_percentage:.2f}% of total\")\n",
    "\n",
    "# Calculate total failures and non-failures for each model\n",
    "model_failures = failure_df['model'].value_counts()\n",
    "model_non_failures = non_failure_df['model'].value_counts()\n",
    "\n",
    "# Calculate total counts by vendor\n",
    "vendor_failures = {}\n",
    "vendor_non_failures = {}\n",
    "\n",
    "for vendor in vendors:\n",
    "    vendor_failures[vendor] = model_failures[model_failures.index.str.startswith(vendor[0])].sum()\n",
    "    vendor_non_failures[vendor] = model_non_failures[model_non_failures.index.str.startswith(vendor[0])].sum()\n",
    "\n",
    "# Prepare data for display by vendor\n",
    "vendor_report = []\n",
    "\n",
    "for vendor in vendors:\n",
    "    fail_count = vendor_failures[vendor]\n",
    "    non_fail_count = vendor_non_failures[vendor]\n",
    "    total_count = vendor_data[vendor][\"records\"]\n",
    "    total_percent = vendor_data[vendor][\"percentage\"]\n",
    "    fail_percent = (fail_count / total_count) * 100 if total_count else 0\n",
    "\n",
    "    vendor_report.append({\n",
    "        \"Vendor\": vendor,\n",
    "        \"Failure count\": fail_count,\n",
    "        \"Non-failure count\": non_fail_count,\n",
    "        \"Total count\": total_count,\n",
    "        \"Total percent\": total_percent,  # Keeping as float\n",
    "        \"Fail percent\": fail_percent   # Keeping as float\n",
    "    })\n",
    "print(\"Notes: In failure percentage, 1.00 means 1%, so 0.007687 means 0.007687% failure rate.\")\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "vendor_report_df = pd.DataFrame(vendor_report)\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "vendor_report_df = pd.DataFrame(vendor_report)\n",
    "\n",
    "# Print the note\n",
    "print(\"Notes: In failure percentage, 1.00 means 1%, so 0.007687 means 0.007687% failure rate.\")\n",
    "\n",
    "# Display the DataFrame\n",
    "display(vendor_report_df)\n",
    "\n",
    "# Plotting failure percentages by vendor\n",
    "plt.figure(figsize=(12, 8))\n",
    "barplot = sns.barplot(x='Fail percent', y='Vendor', data=vendor_report_df, palette='viridis')\n",
    "plt.title('Failure Percentage by Vendor')\n",
    "plt.xlabel('Failure Percentage')\n",
    "plt.ylabel('Vendor')\n",
    "\n",
    "# Adding annotations to each bar\n",
    "for p in barplot.patches:\n",
    "    width = p.get_width()\n",
    "    plt.text(x=width + 0.5,\n",
    "             y=p.get_y() + p.get_height() / 2,\n",
    "             s='{:.5f}%'.format(width),\n",
    "             va='center')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Plotting total failure counts by vendor\n",
    "plt.figure(figsize=(12, 8))\n",
    "barplot = sns.barplot(x='Failure count', y='Vendor', data=vendor_report_df, palette='magma')\n",
    "plt.title('Total Failure Count by Vendor')\n",
    "plt.xlabel('Total Failure Count')\n",
    "plt.ylabel('Vendor')\n",
    "\n",
    "# Adding annotations to each bar\n",
    "for p in barplot.patches:\n",
    "    width = p.get_width()\n",
    "    plt.text(x=width + 3,\n",
    "             y=p.get_y() + p.get_height() / 2,\n",
    "             s=int(width),\n",
    "             va='center')\n",
    "\n",
    "plt.show()\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze on SMART attributes\n",
    "### Analyze on critical Stats\n",
    "- Backblaze mentions five stats as better predictors than others. These are 5, 187, 188, 197, 198.They also provide some analysis using these features.\n",
    "- Show some nan stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRITICAL_STATS = [\n",
    "    1,\n",
    "    5,\n",
    "    7,\n",
    "    10,\n",
    "    184,\n",
    "    187,\n",
    "    188,\n",
    "    189,\n",
    "    190,\n",
    "    193,\n",
    "    194,\n",
    "    196,\n",
    "    197,\n",
    "    198,\n",
    "    201,\n",
    "    240,\n",
    "    241,\n",
    "    242,\n",
    "]\n",
    "\n",
    "# Create a copy of the DataFrame\n",
    "df_smart_stats = df.copy()\n",
    "\n",
    "# List of critical stats columns to check for NaNs\n",
    "critical_columns = [f\"smart_{stat}_raw\" for stat in CRITICAL_STATS]\n",
    "\n",
    "# Ensure the columns exist in the DataFrame\n",
    "existing_columns = [col for col in critical_columns if col in df_smart_stats.columns]\n",
    "\n",
    "# Calculate the number of NaN values in each critical column\n",
    "nan_counts = df_smart_stats[existing_columns].isna().sum()\n",
    "\n",
    "# Calculate the percentage of NaN values\n",
    "total_rows = df_smart_stats.shape[0]\n",
    "nan_percentages = (nan_counts / total_rows) * 100\n",
    "\n",
    "# Combine counts and percentages into a DataFrame\n",
    "nan_stats = pd.DataFrame({\n",
    "    'NaN Count': nan_counts,\n",
    "    'NaN Percentage': nan_percentages\n",
    "})\n",
    "\n",
    "# Display the results\n",
    "print(\"Number and percentage of NaN values in critical columns:\")\n",
    "display(nan_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NaN Counts based on manufacturer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of critical SMART attributes\n",
    "CRITICAL_STATS = [\n",
    "    1, 5, 7, 10, 184, 187, 188, 189, 190, 193, 194, 196, 197, 198, 201, 240, 241, 242,\n",
    "]\n",
    "\n",
    "# Generate column names for critical SMART attributes\n",
    "critical_columns = [f\"smart_{stat}_raw\" for stat in CRITICAL_STATS]\n",
    "\n",
    "# Ensure the columns exist in the DataFrame\n",
    "existing_columns = [col for col in critical_columns if col in df.columns]\n",
    "\n",
    "# Generate DataFrames based on manufacturers\n",
    "seagate_df = df[df[\"model\"].str.startswith(\"S\")]\n",
    "hgst_df = df[df[\"model\"].str.startswith(\"HG\")]\n",
    "toshiba_df = df[df[\"model\"].str.startswith(\"T\")]\n",
    "wdc_df = df[df[\"model\"].str.startswith(\"W\")]\n",
    "hitachi_df = df[df[\"model\"].str.startswith(\"Hi\")]\n",
    "\n",
    "# List of manufacturer DataFrames\n",
    "vendor_dfs = {\n",
    "    \"Seagate\": seagate_df,\n",
    "    \"HGST\": hgst_df,\n",
    "    \"Toshiba\": toshiba_df,\n",
    "    \"WDC\": wdc_df,\n",
    "    \"Hitachi\": hitachi_df\n",
    "}\n",
    "\n",
    "# Function to calculate NaN counts and percentages\n",
    "def calculate_nan_stats(df, existing_columns):\n",
    "    total_rows = df.shape[0]\n",
    "    nan_counts = df[existing_columns].isna().sum()\n",
    "    nan_percentages = (nan_counts / total_rows) * 100\n",
    "    return pd.DataFrame({\n",
    "        'NaN Count': nan_counts,\n",
    "        'NaN Percentage': nan_percentages\n",
    "    })\n",
    "\n",
    "# Calculate and print NaN stats for each vendor\n",
    "for vendor, vendor_df in vendor_dfs.items():\n",
    "    print(f\"\\nNaN stats for {vendor}:\")\n",
    "    nan_stats = calculate_nan_stats(vendor_df, existing_columns)\n",
    "    display(nan_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's drop some useless columns to better reflect our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df['failure'].isin([0, 1])]\n",
    "# Drop columns that contain the keyword 'normalized'\n",
    "df = df.drop(columns=[col for col in df.columns if 'normalized' in col])\n",
    "\n",
    "# Dictionary to store DataFrames post each threshold\n",
    "dataframes_post_threshold = {}\n",
    "\n",
    "# Calculate the number of original columns\n",
    "original_column_count = df.shape[1]\n",
    "\n",
    "# Dictionary to hold columns post-threshold application\n",
    "columns_post_threshold = {}\n",
    "\n",
    "# Drop columns where more than 50% of the values are NaN\n",
    "threshold_50 = len(df) * 0.5\n",
    "df_50 = df.dropna(thresh=threshold_50, axis=1)\n",
    "columns_post_threshold['50%'] = df_50.columns\n",
    "\n",
    "# Count and print results for the 50% threshold\n",
    "columns_dropped_50 = original_column_count - len(df_50.columns)\n",
    "print(f\"After 50% threshold:\")\n",
    "print(f\"Total columns retained: {len(df_50.columns)}\")\n",
    "print(f\"Total columns dropped: {columns_dropped_50}\")\n",
    "print(f\"Ratio of columns dropped: {columns_dropped_50 / original_column_count:.2f}\\n\")\n",
    "\n",
    "# Process for different thresholds (25%, 10%, 1%)\n",
    "thresholds = [0.25, 0.10, 0.01]\n",
    "for threshold in thresholds:\n",
    "    threshold_value = len(df) * (1 - threshold)\n",
    "    new_df = df.dropna(thresh=threshold_value, axis=1)\n",
    "    columns_post_threshold[f'{threshold*100:.0f}%'] = new_df.columns\n",
    "    dataframes_post_threshold[f'{threshold*100:.0f}%'] = new_df  # Save each DataFrame\n",
    "    \n",
    "    # Calculate drops and print results for each threshold\n",
    "    columns_dropped = original_column_count - len(new_df.columns)\n",
    "    print(f\"After {threshold*100:.0f}% threshold:\")\n",
    "    print(f\"Total columns retained: {len(new_df.columns)}\")\n",
    "    print(f\"Total columns dropped: {columns_dropped}\")\n",
    "    print(f\"Ratio of columns dropped: {columns_dropped / original_column_count:.2f}\\n\")\n",
    "\n",
    "# Print columns retained for each threshold\n",
    "for key, columns in columns_post_threshold.items():\n",
    "    print(f\"Columns retained at {key} threshold:\")\n",
    "    print(list(columns))\n",
    "    print()  # Add an empty line for better separation\n",
    "\n",
    "# Optionally, access and use the DataFrame retained at the 1% threshold\n",
    "one_percent_df = dataframes_post_threshold['1%']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df_50.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method = 'linear'\n",
    "# interp_dfs = []\n",
    "# total_interpolated = 0\n",
    "\n",
    "# # Specify the columns to interpolate\n",
    "# columns_to_interpolate = ['failure']  # Add any other columns that you want to interpolate\n",
    "\n",
    "# for serial_num, inner_df in new_df.groupby(level=0):\n",
    "#     inner_df = inner_df.droplevel(level=0).asfreq('D')\n",
    "\n",
    "#     # Count the number of NaN values before interpolation\n",
    "#     before_interpolation = inner_df[columns_to_interpolate].isna().sum().sum()\n",
    "    \n",
    "#     # Interpolate only the specified columns\n",
    "#     inner_df[columns_to_interpolate] = inner_df[columns_to_interpolate].interpolate(\n",
    "#         method=method, limit_direction='both', axis=0)\n",
    "    \n",
    "#     # Apply threshold and round\n",
    "#     threshold = 1e-10\n",
    "#     inner_df[columns_to_interpolate] = inner_df[columns_to_interpolate].applymap(\n",
    "#         lambda x: 0 if np.abs(x) < threshold else x).round(0)\n",
    "\n",
    "#     # Count the number of NaN values after interpolation and rounding\n",
    "#     after_interpolation = inner_df[columns_to_interpolate].isna().sum().sum()\n",
    "    \n",
    "#     # Update the total number of interpolated values\n",
    "#     total_interpolated += before_interpolation - after_interpolation\n",
    "    \n",
    "#     # Forward fill and backward fill to handle any remaining NaN values\n",
    "#     inner_df[columns_to_interpolate] = inner_df[columns_to_interpolate].fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "#     # Add the serial number column and reset index\n",
    "#     inner_df['serial_number'] = serial_num\n",
    "#     inner_df = inner_df.reset_index()\n",
    "    \n",
    "#     # Append the processed DataFrame to the list\n",
    "#     interp_dfs.append(inner_df)\n",
    "\n",
    "# # Concatenate all DataFrames\n",
    "# interp_df = pd.concat(interp_dfs, axis=0)\n",
    "\n",
    "# # Set index and sort\n",
    "# new_df = interp_df.set_index(['serial_number', 'date']).sort_index()\n",
    "\n",
    "# # Print the results\n",
    "# print(f'Total interpolated values: {total_interpolated}, '\n",
    "#       f'Percentage of interpolated values: {(total_interpolated / new_df.size) * 100}%')\n",
    "# print(new_df.isna().sum())\n",
    "# # Initialize the dictionary to store p-values\n",
    "# dict1 = {}\n",
    "\n",
    "# for feature in new_df.columns:\n",
    "#     if 'raw' in feature:\n",
    "#         print(f'Distribution of {feature}:')\n",
    "#         print(new_df.groupby('failure')[feature].describe())\n",
    "\n",
    "# # Perform statistical tests on features containing 'raw'\n",
    "# for feature in new_df.columns:\n",
    "#     if 'raw' in feature:\n",
    "#         print(f'Feature: {feature}')\n",
    "\n",
    "#         # Conduct t-test\n",
    "#         _, p_val_ttest = scipy.stats.ttest_ind(new_df[new_df['failure'] == 0][feature],\n",
    "#                                                new_df[new_df['failure'] == 1][feature], \n",
    "#                                                axis=0, nan_policy='omit')\n",
    "#         print(f'T-test p-value for {feature}: {p_val_ttest:.6f}')\n",
    "\n",
    "#         # Conduct Mann-Whitney U test\n",
    "#         _, p_val_mannu = scipy.stats.mannwhitneyu(new_df[new_df['failure'] == 0][feature], \n",
    "#                                                   new_df[new_df['failure'] == 1][feature], \n",
    "#                                                   alternative='two-sided')\n",
    "#         print(f'Mann-Whitney U test p-value for {feature}: {p_val_mannu:.6f}')\n",
    "\n",
    "#         # Store p-values in the dictionary\n",
    "#         dict1[feature] = {'T-test': p_val_ttest, 'Mann-Whitney U': p_val_mannu}\n",
    "\n",
    "# # Sort features based on the p-values of t-tests\n",
    "# sorted_features_t = {k: v for k, v in sorted(dict1.items(), key=lambda item: item[1]['T-test'])}\n",
    "# sorted_features_mannu = {k: v for k, v in sorted(dict1.items(), key=lambda item: item[1]['Mann-Whitney U'])}\n",
    "\n",
    "# # Convert dictionary to DataFrame and display all features\n",
    "# features_df = pd.DataFrame(sorted_features_t).transpose()\n",
    "# print('Features sorted by T-test p-values:')\n",
    "# print(features_df)\n",
    "\n",
    "# features_df = pd.DataFrame(sorted_features_mannu).transpose()\n",
    "# print('Features sorted by Mann-Whitney U p-values:')\n",
    "# print(features_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now let's do some basic statical things: (dropnas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is your original DataFrame\n",
    "total_rows_before = len(new_df)\n",
    "\n",
    "# Drop rows where any column has NaN values\n",
    "clean_df = new_df.dropna()\n",
    "\n",
    "# Calculate the total number of rows after dropping\n",
    "total_rows_after = len(clean_df)\n",
    "\n",
    "# Calculate the percentage of rows dropped\n",
    "percentage_dropped = ((total_rows_before - total_rows_after) / total_rows_before) * 100\n",
    "\n",
    "print(f\"Percentage of rows dropped: {percentage_dropped:.2f}%\")\n",
    "\n",
    "print(clean_df.isna().sum())\n",
    "# Initialize the dictionary to store p-values\n",
    "dict1 = {}\n",
    "\n",
    "for feature in clean_df.columns:\n",
    "    if 'raw' in feature:\n",
    "        print(f'Distribution of {feature}:')\n",
    "        print(new_df.groupby('failure')[feature].describe())\n",
    "\n",
    "# Perform statistical tests on features containing 'raw'\n",
    "for feature in clean_df.columns:\n",
    "    if 'raw' in feature:\n",
    "        print(f'Feature: {feature}')\n",
    "\n",
    "        # Conduct t-test\n",
    "        _, p_val_ttest = scipy.stats.ttest_ind(clean_df[clean_df['failure'] == 0][feature],\n",
    "                                               clean_df[clean_df['failure'] == 1][feature], \n",
    "                                               axis=0, nan_policy='omit')\n",
    "        print(f'T-test p-value for {feature}: {p_val_ttest:.6f}')\n",
    "\n",
    "        # Conduct Mann-Whitney U test\n",
    "        _, p_val_mannu = scipy.stats.mannwhitneyu(clean_df[clean_df['failure'] == 0][feature], \n",
    "                                                  clean_df[clean_df['failure'] == 1][feature], \n",
    "                                                  alternative='two-sided')\n",
    "        print(f'Mann-Whitney U test p-value for {feature}: {p_val_mannu:.6f}')\n",
    "\n",
    "        # Store p-values in the dictionary\n",
    "        dict1[feature] = {'T-test': p_val_ttest, 'Mann-Whitney U': p_val_mannu}\n",
    "\n",
    "# Sort features based on the p-values of t-tests\n",
    "sorted_features_t = {k: v for k, v in sorted(dict1.items(), key=lambda item: item[1]['T-test'])}\n",
    "sorted_features_mannu = {k: v for k, v in sorted(dict1.items(), key=lambda item: item[1]['Mann-Whitney U'])}\n",
    "\n",
    "# Convert dictionary to DataFrame and display all features\n",
    "features_df = pd.DataFrame(sorted_features_t).transpose()\n",
    "print('Features sorted by T-test p-values:')\n",
    "print(features_df)\n",
    "\n",
    "features_df = pd.DataFrame(sorted_features_mannu).transpose()\n",
    "print('Features sorted by Mann-Whitney U p-values:')\n",
    "print(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rows to use for plotting - sampling is done if the number of rows is too much to keep in memory\n",
    "num_rows_to_sample = 5000000\n",
    "hist_cols = new_df.columns  # Removed the filtering on NaN percentage\n",
    "len_df = len(new_df)\n",
    "\n",
    "for col in hist_cols:\n",
    "    # Get all the values for this column\n",
    "    if num_rows_to_sample < len_df:\n",
    "        data = new_df[col].sample(frac=num_rows_to_sample / len_df)\n",
    "    else:\n",
    "        data = new_df[col]\n",
    "\n",
    "    # Print the number of NaN values and plot only the non-null values\n",
    "    print(\n",
    "        data.isna().sum(),\n",
    "        \"out of\",\n",
    "        data.shape[0],\n",
    "        \"are NaN values. These are not shown on the graph below\",\n",
    "    )\n",
    "    \n",
    "    # If no non-NaN data is available, skip plotting\n",
    "    if data.notna().sum() > 0:\n",
    "        sns.histplot(data.dropna(), kde=True)  # Using histplot for histograms with KDE\n",
    "        plt.title(f'Histogram of {col}')  # Optional: Add title for clarity\n",
    "        plt.xlabel(f'Values of {col}')  # Optional: Label the x-axis\n",
    "        plt.ylabel('Frequency')  # Optional: Label the y-axis\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No non-NaN data available in {col}. Skipping histogram.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define correlation columns: including 'failure', 'raw'\n",
    "corr_cols = [\"failure\"] + [col for col in one_percent_df.columns if 'raw' in col]\n",
    "# Downsampling serial numbers from both failed and working sets\n",
    "downsampled_sers = (\n",
    "    failure_df.sample(n=min(500, len(failure_df))).values.tolist() +\n",
    "    non_failure_df.sample(n=min(75000, len(non_failure_df))).values.tolist()\n",
    ")\n",
    "print(failure_df)\n",
    "print(non_failure_df)\n",
    "\n",
    "# Reset the index to turn 'serial_number' into a column\n",
    "one_percent_df_reset = one_percent_df.reset_index()\n",
    "one_percent_df_reset.drop(columns=[\"model\"])\n",
    "print(one_percent_df_reset.columns)\n",
    "corr_mat = one_percent_df_reset[one_percent_df_reset[\"serial_number\"].isin(downsampled_sers)][corr_cols].corr()\n",
    "print(corr_mat)\n",
    "\n",
    "# Visualization with a heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(\n",
    "    corr_mat,\n",
    "    ax=ax,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    cmap=\"RdBu\",\n",
    "    cbar_kws={\"shrink\": 0.5},\n",
    ")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of manufacturer DataFrame names for dynamic access\n",
    "manufacturers_data = {\n",
    "    'Seagate': seagate_df,\n",
    "    'WDC': wdc_df,\n",
    "    'HGST': hgst_df,\n",
    "    'Hitachi': hitachi_df,\n",
    "    'Toshiba': toshiba_df\n",
    "}\n",
    "\n",
    "NAN_PERCENT_THRESHOLD = 0.01  # Define your NAN percent threshold\n",
    "\n",
    "# Calculate the percentage of NaNs for each manufacturer's DataFrame and store it\n",
    "manufacturers_nan_ct = {name: data.isna().mean() for name, data in manufacturers_data.items()}\n",
    "\n",
    "# Loop over each manufacturer\n",
    "for name, data_df in manufacturers_data.items():\n",
    "    nan_ct = manufacturers_nan_ct[name]\n",
    "\n",
    "    # Determine the correlation columns based on the NAN percent threshold\n",
    "    corr_cols = [\"failure\"] + list(\n",
    "        nan_ct[nan_ct < NAN_PERCENT_THRESHOLD].index\n",
    "    )\n",
    "\n",
    "    # Check if downsampling serial numbers are needed\n",
    "    if 'downsampled_sers' in locals():\n",
    "        data_df = data_df[data_df[\"serial_number\"].isin(downsampled_sers)]\n",
    "\n",
    "    # Compute the correlation matrix\n",
    "    corr_mat = data_df[corr_cols].corr()\n",
    "\n",
    "    # Plotting the heatmap\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    sns.heatmap(\n",
    "        corr_mat,\n",
    "        ax=ax,\n",
    "        vmin=-1,\n",
    "        vmax=1,\n",
    "        square=True,\n",
    "        linewidths=0.5,\n",
    "        cmap=\"RdBu\",\n",
    "        cbar_kws={\"shrink\": 0.5},\n",
    "    )\n",
    "    ax.set_title(f'Correlation Matrix for {name} Drives')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Might be better to call compute to get the combined data of all serials in subset\n",
    "# as opposed to calling it for earch serial number in for loop\n",
    "\n",
    "# NOTE: running this cell will take a VERY long time (~1hr on intel i7 w/ 16GB ram)\n",
    "# adjust NUM_DRIVES_TO_SAMPLE to select a small subset to use for plotting\n",
    "NUM_DRIVES_TO_SAMPLE = 50\n",
    "\n",
    "# plots for smart stat 5\n",
    "# cols_to_plot = ['smart_5_raw', 'smart_7_raw']\n",
    "\n",
    "cols_to_plot = ['smart_' + str(attr) + '_raw' for attr in CRITICAL_STATS]\n",
    "cols_to_plot.remove(\"smart_201_raw\")  # too many nans\n",
    "cols_to_plot.remove(\"smart_201_normalized\")  # too many nans\n",
    "\n",
    "# one figure per smart stat\n",
    "figs = [plt.figure(i, figsize=(10, 10)) for i in range(len(cols_to_plot))]\n",
    "axes = [f.add_subplot(111) for f in figs]\n",
    "for colname, ax in zip(cols_to_plot, axes):\n",
    "    ax.set_title(\n",
    "        \"{} vs Time for {} Failed Drives\".format(colname, NUM_DRIVES_TO_SAMPLE)\n",
    "    )\n",
    "    ax.set_xlabel(\"Number of Days\")\n",
    "    ax.set_ylabel(colname)\n",
    "\n",
    "# keep track of what hard drives were used to generate the data. NOTE: only the first 16 chars of ser will be saved\n",
    "failed_ser_subset = np.empty(shape=(NUM_DRIVES_TO_SAMPLE), dtype=\"<S16\")\n",
    "\n",
    "# make the figures\n",
    "for i, ser in enumerate(\n",
    "    failure_df[\"serial_number\"].sample(NUM_DRIVES_TO_SAMPLE, random_state=42)\n",
    "):\n",
    "    # log serial numbers which are being used\n",
    "    failed_ser_subset[i] = ser\n",
    "    print(\"{} / {}. Drive serial number {}\".format(i + 1, NUM_DRIVES_TO_SAMPLE, ser))\n",
    "\n",
    "    # get teh data to make the figures\n",
    "    drive_data = df[df[\"serial_number\"] == ser][cols_to_plot].compute()\n",
    "\n",
    "    # dummy x axis data\n",
    "    xvals = [i for i in range(drive_data.shape[0])]\n",
    "\n",
    "    # make the plot\n",
    "    for ax, c in zip(axes, cols_to_plot):\n",
    "        ax.plot(xvals, drive_data[c])\n",
    "\n",
    "# save the figures\n",
    "for f, c in zip(figs, cols_to_plot):\n",
    "    f.savefig(\"output/{}_failed.png\".format(c))\n",
    "\n",
    "# save the serial numbres used in figures\n",
    "np.save(\"failed_graphs_serials\", failed_ser_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA analysis\n",
    "- For each group, which corresponds to a unique serial number (hence a unique hard drive), the mean and standard deviation of each column are computed. This helps in summarizing the time-series data into statistical features which are easier to analyze.\n",
    "- The aggregated mean and standard deviation DataFrames are concatenated side by side. This results in a DataFrame where each row represents a hard drive, and each column represents a mean or standard deviation of a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df_50.groupby(\"serial_number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each group, which corresponds to a unique serial number (hence a unique hard drive), the mean and standard deviation of each column are computed. This helps in summarizing the time-series data into statistical features which are easier to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_means = groups.mean().compute().add_prefix(\"mean_\")\n",
    "group_stds = groups.std().compute().add_prefix(\"std_\")\n",
    "group_stats = pd.concat([group_means, group_stds], axis=1)\n",
    "group_stats[\"failure\"] = group_stats[\"serial_number\"].isin(failure_df[\"serial_number\"])\n",
    "group_stats.head()\n",
    "clean_group_stats = group_stats.dropna(how=\"any\") # Drop rows with any NaN values\n",
    "\n",
    "# make sure we still have enough failure drive data\n",
    "print(clean_group_stats[\"failure\"].sum(), \"failed drives data retained\")\n",
    "clean_group_stats[clean_group_stats[\"failure\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data and find the top principal components\n",
    "scaler = StandardScaler()\n",
    "pca = PCA(n_components=3, random_state=42, whiten=True).fit_transform(\n",
    "    scaler.fit_transform(clean_group_stats.drop([\"serial_number\", \"failure\"], axis=1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot pca\n",
    "colors = [\"blue\", \"red\"]\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.scatter(\n",
    "    pca[:, 0],\n",
    "    pca[:, 1],\n",
    "    pca[:, 2],\n",
    "    c=clean_group_stats[\"failure\"],\n",
    "    cmap=matplotlib.colors.ListedColormap(colors),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
